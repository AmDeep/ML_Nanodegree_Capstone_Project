{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import utils\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import datasets\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "from keras import losses\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Dense    #for Dense layers\n",
    "from keras.layers import BatchNormalization #for batch normalization\n",
    "from keras.layers import Dropout            #for random dropout\n",
    "from keras.models import Sequential #for sequential implementation\n",
    "from keras.optimizers import Adam   #for adam optimizer\n",
    "from keras import regularizers      #for l2 regularization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and parsing Dataset 1\n",
    "# Data Set:- FD001\n",
    "# Train trjectories:- 100\n",
    "# Test trajectories:- 100\n",
    "# Conditions:- ONE (Sea Level)\n",
    "# Fault Modes:- ONE (HPC Degradation)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_train_1 = pd.read_csv('C:/Users/Amardeep/Downloads/ML Nanodegree/PROJECT3/CMAPSSData/train_FD001.txt', delimiter=' ', header=None)\n",
    "data_train_1.columns = [\"unit number\", \"time_cycles\", \"op_set_1\", \"op_set_2\",\"op_set_3\",\"sm_1\",\"sm_2\",\"sm_3\",\"sm_4\",\"sm_5\",\"sm_6\",\"sm_7\",\"sm_8\",\"sm_9\",\"sm_10\",\"sm_11\",\"sm_12\",\"sm_13\",\"sm_14\",\"sm_15\",\"sm_16\",\"sm_17\",\"sm_18\",\"sm_19\",\"sm_20\",\"sm_21\",\"sm_22\",\"sm_23\"]\n",
    "data_train_1=data_train_1.drop(columns=['sm_22','sm_23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_1 = pd.read_csv('C:/Users/Amardeep/Downloads/ML Nanodegree/PROJECT3/CMAPSSData/test_FD001.txt', sep=\" \", header=None)\n",
    "data_test_1.columns = [\"unit number\", \"time_cycles\", \"op_set_1\", \"op_set_2\",\"op_set_3\",\"sm_1\",\"sm_2\",\"sm_3\",\"sm_4\",\"sm_5\",\"sm_6\",\"sm_7\",\"sm_8\",\"sm_9\",\"sm_10\",\"sm_11\",\"sm_12\",\"sm_13\",\"sm_14\",\"sm_15\",\"sm_16\",\"sm_17\",\"sm_18\",\"sm_19\",\"sm_20\",\"sm_21\",\"sm_22\",\"sm_23\"]\n",
    "data_test_1=data_test_1.drop(columns=['sm_22','sm_23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing RULs for all datasets\n",
    "RUL_1=pd.read_csv('C:/Users/Amardeep/Downloads/ML Nanodegree/PROJECT3/CMAPSSData/RUL_FD001.txt', sep=\" \", header=None)\n",
    "RUL_1.columns = [\"RUL\",\"d\"]\n",
    "RUL_1=RUL_1.drop(columns=['d'])\n",
    "RUL_2=pd.read_csv('C:/Users/Amardeep/Downloads/ML Nanodegree/PROJECT3/CMAPSSData/RUL_FD002.txt', sep=\" \", header=None)\n",
    "RUL_2.columns = [\"RUL\",\"d\"]\n",
    "RUL_2=RUL_2.drop(columns=['d'])\n",
    "RUL_3=pd.read_csv('C:/Users/Amardeep/Downloads/ML Nanodegree/PROJECT3/CMAPSSData/RUL_FD003.txt', sep=\" \", header=None)\n",
    "RUL_3.columns = [\"RUL\",\"d\"]\n",
    "RUL_3=RUL_3.drop(columns=['d'])\n",
    "RUL_4=pd.read_csv('C:/Users/Amardeep/Downloads/ML Nanodegree/PROJECT3/CMAPSSData/RUL_FD004.txt', sep=\" \", header=None)\n",
    "RUL_4.columns = [\"RUL\",\"d\"]\n",
    "RUL_4=RUL_4.drop(columns=['d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new column for RUL to match it with the test dataframe.\n",
    "RUL_1['unit number']= 1+RUL_1['RUL'].index.values\n",
    "#Merging common columns based on unit number.\n",
    "data_test_1=pd.merge(data_test_1, RUL_1, on='unit number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to create RUL values for the training set.\n",
    "def create_RUL(data, factor = 0):\n",
    "    df = data.copy()\n",
    "    RUL = df.groupby('unit number')['time_cycles'].max().reset_index()\n",
    "    RUL = pd.DataFrame(RUL)\n",
    "    RUL.columns = ['unit number','max']\n",
    "    df = df.merge(RUL, on=['unit number'], how='left')\n",
    "    df['RUL'] = df['max'] - df['time_cycles']\n",
    "    df.drop(columns=['max'],inplace = True)\n",
    "    \n",
    "    return df[df['time_cycles'] > factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= create_RUL(data_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading features generated from training set:\n",
    "wvt_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\trainwvt1.csv')\n",
    "wpsd_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\trainwpsd1.csv')\n",
    "opsd_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\trainopsd1.csv')\n",
    "vf_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\trainvf1.csv')\n",
    "fft_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\fft_train_1.csv')\n",
    "ae_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\ae_train_1.csv')\n",
    "ip_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\ip_train_1.csv')\n",
    "iff_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\iff_train_1.csv')\n",
    "tempshift_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\tempshift_train_1.csv')\n",
    "srtempshift_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\srtempshift_train_1.csv')\n",
    "rollshift_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\rollshift_train_1.csv')\n",
    "ac_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\ac_train_1.csv')\n",
    "cwtt_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\cwtt_train_1.csv')\n",
    "idwt_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\idwt_train_1.csv')\n",
    "lpf_train=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\train\\lpf_train_1.csv')\n",
    "#Loading features generated from testing set:\n",
    "wvt_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\testwvt1.csv')\n",
    "wpsd_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\testwpsd1.csv')\n",
    "opsd_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\testopsd1.csv')\n",
    "vf_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\testvf1.csv')\n",
    "fft_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\fft_test_1.csv')\n",
    "ae_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\ae_test_1.csv')\n",
    "ip_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\ip_test_1.csv')\n",
    "iff_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\iff_test_1.csv')\n",
    "tempshift_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\tempshift_test_1.csv')\n",
    "srtempshift_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\srtempshift_test_1.csv')\n",
    "rollshift_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\rollshift_test_1.csv')\n",
    "ac_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\ac_test_1.csv')\n",
    "cwtt_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\cwtt_test_1.csv')\n",
    "idwt_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\idwt_test_1.csv')\n",
    "lpf_test=pd.read_csv(r'C:\\Users\\Amardeep\\Downloads\\ML Nanodegree\\PROJECT3\\WVT\\lpf_test_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining features with training set\n",
    "new_df_train = pd.concat([df_train, wvt_train,wpsd_train, opsd_train,vf_train,fft_train,ae_train,ip_train,iff_train,tempshift_train,srtempshift_train,rollshift_train,cwtt_train,idwt_train,lpf_train], axis=1)\n",
    "#Combining features with testing set\n",
    "new_df_test= pd.concat([data_test_1, wvt_test,wpsd_test, opsd_test,vf_test,fft_test,ae_test,ip_test,iff_test,tempshift_test,srtempshift_test,rollshift_test,cwtt_test,idwt_test,lpf_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping NaN values and filling remaining values with 0\n",
    "new_df_train=new_df_train.dropna(how=\"all\")\n",
    "new_df_train=new_df_train.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling NaN values in both training and testing datasets\n",
    "new_df_train=new_df_train.fillna(0)\n",
    "new_df_test=new_df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=new_df_train.drop(columns=['RUL'])\n",
    "y_train=new_df_train['RUL']\n",
    "X_test=new_df_test.drop(columns=['RUL'])\n",
    "y_test=new_df_test['RUL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Remove uncommon column in both datasets\n",
    "#Create separate lists for training and testing dataset columns.\n",
    "traincolumns=X_train.columns\n",
    "testcolumns=X_test.columns\n",
    "trainc=traincolumns.values.tolist()\n",
    "testc=testcolumns.values.tolist()\n",
    "def Diff(li1, li2): \n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2] \n",
    "    return li_dif \n",
    "  \n",
    "# Driver Code \n",
    "list1 = Diff(testc, trainc) \n",
    "print(list1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.drop(columns=['fft_test_1', 'cwt_op_Set_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20632, 277)\n",
      "(53760, 277)\n",
      "(20632,)\n",
      "(53760,)\n"
     ]
    }
   ],
   "source": [
    "#Confirm the shape of the generated new dataframes.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 1\n",
    "test_size=0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43008 train samples\n",
      "There are 10752 test samples\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {X_train.shape[0]} train samples')\n",
    "print(f'There are {X_test.shape[0]} test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10752, 279)\n",
      "(10752,)\n"
     ]
    }
   ],
   "source": [
    "train_inputs = X_test.values\n",
    "train_targets = y_test.values\n",
    "print(train_inputs.shape)\n",
    "print(train_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = train_inputs.shape[1]\n",
    "input_shape = (n_cols, )\n",
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate = 0.01, activation = 'relu'):\n",
    "  \n",
    "    # Create an Adam optimizer with the given learning rate\n",
    "    opt = Adam(lr=learning_rate)\n",
    "  \n",
    "    #l  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, \n",
    "                    activation = activation,\n",
    "                    input_shape = input_shape,\n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(128,\n",
    "                    activation = activation, \n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1, activation = activation))\n",
    "# Compile the model\n",
    "    model.compile(optimizer = opt,\n",
    "                  loss = \"mean_absolute_error\",\n",
    "                  metrics=['mse', \"mape\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KerasClassifier object\n",
    "model = KerasRegressor(build_fn = create_model,\n",
    "                       verbose = 0)\n",
    "# Define the hyperparameter space\n",
    "params = {'activation': [\"relu\", \"tanh\"],\n",
    "          'batch_size': [16, 32, 64], \n",
    "          'epochs': [50, 100],\n",
    "          'learning_rate': [0.01, 0.001, 0.0001]}\n",
    "# Create a randomize search cv object \n",
    "random_search = RandomizedSearchCV(model,\n",
    "                                   param_distributions = params,\n",
    "                                   cv = KFold(10))\n",
    "random_search_results = random_search.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -16.184173959346843 and Best Params:  {'learning_rate': 0.0001, 'epochs': 100, 'batch_size': 16, 'activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: \",\n",
    "      random_search_results.best_score_,\n",
    "      \"and Best Params: \",\n",
    "      random_search_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = train_inputs.shape[1]\n",
    "input_shape = (n_cols, )\n",
    "# Create the model object with default arguments\n",
    "def create_model(learning_rate = 0.0001, activation='tanh'):\n",
    "  \n",
    "    # Set Adam optimizer with the given learning rate\n",
    "    opt = Adam(lr = learning_rate)\n",
    "  \n",
    "    # Create your binary classification model  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128,\n",
    "                    activation = activation,\n",
    "                    input_shape = input_shape,\n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(128,\n",
    "                    activation = activation, \n",
    "                    activity_regularizer = regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1, activation = activation))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = opt,\n",
    "                  loss = \"mean_absolute_error\",\n",
    "                  metrics = ['mse', \"mape\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy was: -16.183890022436934\n",
      "With a standard deviation of: 0.863807563878001\n"
     ]
    }
   ],
   "source": [
    "#Reevaluating with best parameters\n",
    "model = KerasRegressor(build_fn = create_model,\n",
    "                       epochs = 100, \n",
    "                       batch_size = 16,\n",
    "                       verbose = 0)\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model,\n",
    "                         train_inputs,\n",
    "                         train_targets,\n",
    "                         cv = 10)\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 100, 'batch_size': 16, 'verbose': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sk_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseWrapper.filter_sk_params of <keras.wrappers.scikit_learn.KerasRegressor object at 0x0000024AB7DCF108>>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.filter_sk_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseWrapper.fit of <keras.wrappers.scikit_learn.KerasRegressor object at 0x0000024AB7DCF108>>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method KerasRegressor.score of <keras.wrappers.scikit_learn.KerasRegressor object at 0x0000024AB7DCF108>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24a81c46f08>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save('saved_kerasregressor_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_334/kernel:0' shape=(279, 128) dtype=float32, numpy=\n",
       " array([[ 0.10412333, -0.09738728, -0.00933898, ...,  0.00058707,\n",
       "         -0.01583734, -0.17997298],\n",
       "        [-0.09125005,  0.02386453, -0.03660853, ..., -0.01521669,\n",
       "         -0.03074618, -0.12718363],\n",
       "        [ 0.01966858,  0.06339759, -0.10620739, ...,  0.08978471,\n",
       "         -0.12096994,  0.09963976],\n",
       "        ...,\n",
       "        [-0.02484225, -0.03704802, -0.05872082, ...,  0.04288492,\n",
       "         -0.07967255,  0.10123626],\n",
       "        [-0.00219815, -0.10300548,  0.07577722, ..., -0.08634397,\n",
       "         -0.04395959, -0.05524709],\n",
       "        [-0.07008818,  0.05625782, -0.06017948, ..., -0.11712108,\n",
       "          0.09684663,  0.0223255 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_334/bias:0' shape=(128,) dtype=float32, numpy=\n",
       " array([ 1.99530255e-02, -5.64282946e-03,  8.97656195e-03,  3.62281944e-03,\n",
       "        -8.48817732e-03, -1.13582006e-02,  3.80437938e-04, -1.09038940e-02,\n",
       "        -1.82823986e-02, -7.07688602e-03,  1.61736328e-02,  1.74017753e-02,\n",
       "        -1.69827032e-03,  8.03954713e-03,  0.00000000e+00, -5.57812303e-03,\n",
       "        -2.19592750e-02, -1.96705889e-02,  2.40516346e-02,  0.00000000e+00,\n",
       "         1.49828044e-03,  2.16489076e-03,  1.10589573e-02,  1.69399362e-02,\n",
       "         1.26199680e-03, -4.35784721e-04, -3.36482376e-03, -4.19478965e-06,\n",
       "        -1.97866485e-02, -2.59132730e-03, -7.98510519e-05, -1.59777347e-02,\n",
       "        -1.03554679e-02,  2.57369801e-02,  2.55992096e-02,  9.65612754e-03,\n",
       "        -7.05992058e-03,  0.00000000e+00, -1.60146728e-02,  1.03435675e-02,\n",
       "        -1.34441713e-02,  1.63158346e-02,  5.83098845e-05,  3.27727804e-03,\n",
       "        -1.88022945e-02,  1.23270927e-02,  1.25107504e-02,  8.15817900e-03,\n",
       "        -5.60274022e-03,  8.10642634e-03,  7.24621164e-03, -1.00828884e-02,\n",
       "         5.01182629e-03, -8.24866723e-03,  2.33776728e-03,  8.42389651e-03,\n",
       "        -8.99678282e-03, -2.37221960e-02, -9.59414523e-03,  8.61688051e-03,\n",
       "         2.29727644e-02, -3.17260018e-03,  7.33261788e-03,  0.00000000e+00,\n",
       "         4.80498467e-03,  1.28106363e-02, -1.45772379e-02, -8.38202983e-03,\n",
       "        -9.61221231e-04, -8.17570928e-03, -4.14587464e-03,  0.00000000e+00,\n",
       "         2.13977732e-02, -5.01150591e-03, -9.64032114e-03, -9.02892090e-03,\n",
       "        -7.72253145e-03,  1.69562809e-02,  0.00000000e+00,  4.27151471e-03,\n",
       "        -1.79457664e-02,  6.29309937e-03,  2.06816811e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00995079e-02, -2.44622661e-05, -6.61178073e-03,\n",
       "        -2.09655357e-03, -1.40167391e-02, -7.81004177e-03,  2.24754661e-02,\n",
       "         1.35236531e-02, -6.72215674e-06,  3.50011885e-03,  2.06045397e-02,\n",
       "         1.42478924e-02,  1.98349846e-03, -1.42174605e-02, -3.05702793e-03,\n",
       "         3.83819104e-04, -1.59037262e-02, -1.22981356e-03, -9.65038594e-03,\n",
       "         2.67132884e-03, -1.93215813e-02, -1.06798508e-03,  1.43182939e-02,\n",
       "         4.38445248e-03,  3.94410221e-03,  9.98669863e-03, -3.58247757e-03,\n",
       "         2.16725864e-03,  2.91057788e-02,  2.03662906e-02, -1.08862855e-02,\n",
       "        -7.08449632e-03, -6.27620902e-04,  1.06514255e-02,  1.68005787e-02,\n",
       "         3.38997017e-03,  4.92408499e-03,  0.00000000e+00,  1.04423547e-02,\n",
       "         3.67398113e-02,  2.02542706e-03,  1.14139691e-02, -1.37731722e-02],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_335/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
       " array([[-0.09548204,  0.03432821,  0.0469967 , ..., -0.01200852,\n",
       "          0.08514043,  0.02518556],\n",
       "        [-0.0984121 ,  0.04379791, -0.05961775, ...,  0.07311657,\n",
       "         -0.02783873, -0.05714569],\n",
       "        [-0.03955619,  0.0172625 , -0.08537827, ..., -0.09657539,\n",
       "         -0.00054019,  0.05780836],\n",
       "        ...,\n",
       "        [ 0.03415758,  0.09002019,  0.06056491, ..., -0.0012689 ,\n",
       "          0.03869045, -0.01576398],\n",
       "        [ 0.06153649, -0.02455719, -0.01043156, ...,  0.00168154,\n",
       "          0.03292063,  0.03570147],\n",
       "        [-0.04274302, -0.05854054, -0.11127185, ..., -0.2065419 ,\n",
       "          0.02686951,  0.22550943]], dtype=float32)>,\n",
       " <tf.Variable 'dense_335/bias:0' shape=(128,) dtype=float32, numpy=\n",
       " array([-0.10536565, -0.1334265 ,  0.06327215, -0.10261533, -0.11150515,\n",
       "         0.09714984,  0.07468999,  0.07657605,  0.12950343, -0.1060369 ,\n",
       "         0.13144453, -0.05811827,  0.10712688, -0.190704  ,  0.11146745,\n",
       "         0.09932531, -0.03344205,  0.05982385, -0.1477404 , -0.07337822,\n",
       "         0.11294744,  0.01401081,  0.16458105, -0.11607424, -0.01375248,\n",
       "         0.14653586,  0.07538236,  0.1578102 ,  0.15111156,  0.06264955,\n",
       "         0.10099067, -0.14096326, -0.12752114,  0.05483899, -0.105965  ,\n",
       "         0.10125683,  0.09873208,  0.08387406, -0.07701245,  0.13179268,\n",
       "         0.05778766, -0.06488324,  0.1789158 ,  0.16975881,  0.11772667,\n",
       "         0.09906489,  0.12464455,  0.0213902 , -0.08617   ,  0.0904461 ,\n",
       "         0.07433901,  0.11339112,  0.05061619, -0.16427879, -0.08655355,\n",
       "        -0.15079126, -0.08267609, -0.11851324, -0.15622514, -0.18598673,\n",
       "        -0.12984644, -0.04607574, -0.11639711,  0.08431679, -0.15877877,\n",
       "         0.06553699,  0.09440674, -0.13947368,  0.12640464, -0.17276567,\n",
       "         0.0962355 ,  0.09486038, -0.08499598,  0.08309603,  0.10536118,\n",
       "        -0.09638619, -0.08720297,  0.04356494,  0.02760067,  0.18134414,\n",
       "        -0.05632732, -0.00874771, -0.16429758, -0.07250176,  0.18951276,\n",
       "         0.130448  , -0.00433583,  0.12261477,  0.01338804,  0.08446626,\n",
       "         0.11072027, -0.15790032, -0.04087012,  0.01783641,  0.01881559,\n",
       "         0.06361219, -0.09611642, -0.11670127,  0.09740116, -0.14829957,\n",
       "        -0.13780499,  0.11994424,  0.1727259 ,  0.09805062,  0.13884011,\n",
       "        -0.22299175, -0.05774802,  0.07509191, -0.10650432, -0.07483712,\n",
       "        -0.14284517, -0.0889005 , -0.13817298, -0.12255003,  0.06114699,\n",
       "        -0.09983955,  0.17814532, -0.12117312, -0.12637916,  0.12200391,\n",
       "        -0.07063369, -0.1391674 ,  0.13263161,  0.11435922,  0.13396172,\n",
       "        -0.16121322,  0.09780357,  0.10676972], dtype=float32)>,\n",
       " <tf.Variable 'dense_336/kernel:0' shape=(128, 1) dtype=float32, numpy=\n",
       " array([[-0.00475676],\n",
       "        [-0.00376158],\n",
       "        [ 0.01126972],\n",
       "        [-0.00523195],\n",
       "        [ 0.01396602],\n",
       "        [ 0.0057793 ],\n",
       "        [ 0.00461279],\n",
       "        [ 0.00736542],\n",
       "        [-0.0145261 ],\n",
       "        [-0.00565074],\n",
       "        [ 0.00496572],\n",
       "        [-0.01217595],\n",
       "        [ 0.00743466],\n",
       "        [-0.00709051],\n",
       "        [-0.01396429],\n",
       "        [ 0.00334179],\n",
       "        [ 0.01358692],\n",
       "        [ 0.00524139],\n",
       "        [ 0.01497652],\n",
       "        [-0.00412019],\n",
       "        [ 0.00537877],\n",
       "        [-0.0123996 ],\n",
       "        [-0.01520372],\n",
       "        [-0.00793747],\n",
       "        [-0.01398443],\n",
       "        [ 0.00908231],\n",
       "        [ 0.01531815],\n",
       "        [ 0.00634925],\n",
       "        [-0.01582094],\n",
       "        [ 0.01286361],\n",
       "        [-0.01527814],\n",
       "        [ 0.01683415],\n",
       "        [-0.00346905],\n",
       "        [ 0.00419744],\n",
       "        [-0.00616861],\n",
       "        [ 0.00396551],\n",
       "        [ 0.00437216],\n",
       "        [-0.01387938],\n",
       "        [ 0.01556006],\n",
       "        [ 0.00648749],\n",
       "        [ 0.00122809],\n",
       "        [-0.00481499],\n",
       "        [ 0.00811116],\n",
       "        [ 0.00804028],\n",
       "        [ 0.00485056],\n",
       "        [ 0.00486975],\n",
       "        [ 0.00597663],\n",
       "        [ 0.01049305],\n",
       "        [-0.00347305],\n",
       "        [-0.01580716],\n",
       "        [ 0.00431421],\n",
       "        [ 0.00456784],\n",
       "        [ 0.00665022],\n",
       "        [-0.00763049],\n",
       "        [ 0.01352149],\n",
       "        [-0.00840896],\n",
       "        [-0.00272774],\n",
       "        [-0.00428136],\n",
       "        [-0.00710138],\n",
       "        [-0.00818621],\n",
       "        [ 0.01482482],\n",
       "        [ 0.0137044 ],\n",
       "        [-0.0037525 ],\n",
       "        [ 0.00799383],\n",
       "        [ 0.01436767],\n",
       "        [ 0.01075038],\n",
       "        [ 0.00919736],\n",
       "        [-0.00615911],\n",
       "        [ 0.00644651],\n",
       "        [-0.00750249],\n",
       "        [ 0.0041575 ],\n",
       "        [ 0.00611935],\n",
       "        [-0.01040534],\n",
       "        [ 0.00387769],\n",
       "        [ 0.00885346],\n",
       "        [-0.00477246],\n",
       "        [-0.0052726 ],\n",
       "        [-0.0148478 ],\n",
       "        [-0.0160923 ],\n",
       "        [ 0.00882427],\n",
       "        [-0.00928692],\n",
       "        [-0.01195958],\n",
       "        [-0.00895655],\n",
       "        [-0.00606748],\n",
       "        [ 0.00619349],\n",
       "        [ 0.00747631],\n",
       "        [-0.01056442],\n",
       "        [-0.01605384],\n",
       "        [ 0.01128705],\n",
       "        [ 0.00580138],\n",
       "        [ 0.00763858],\n",
       "        [-0.0084481 ],\n",
       "        [-0.012913  ],\n",
       "        [ 0.0135926 ],\n",
       "        [ 0.01272327],\n",
       "        [-0.01283356],\n",
       "        [-0.00400773],\n",
       "        [-0.00901321],\n",
       "        [ 0.00656942],\n",
       "        [ 0.01609863],\n",
       "        [ 0.01497365],\n",
       "        [-0.01471613],\n",
       "        [ 0.00526225],\n",
       "        [ 0.00803721],\n",
       "        [-0.01371609],\n",
       "        [-0.00716897],\n",
       "        [-0.00548584],\n",
       "        [ 0.00586587],\n",
       "        [-0.00417049],\n",
       "        [-0.00509319],\n",
       "        [ 0.01603211],\n",
       "        [-0.00379132],\n",
       "        [-0.00461152],\n",
       "        [-0.00469307],\n",
       "        [ 0.00289471],\n",
       "        [-0.00277872],\n",
       "        [ 0.00832335],\n",
       "        [-0.00290852],\n",
       "        [ 0.01633824],\n",
       "        [ 0.00215642],\n",
       "        [-0.00568362],\n",
       "        [-0.00798429],\n",
       "        [ 0.00574922],\n",
       "        [ 0.00777957],\n",
       "        [-0.01406073],\n",
       "        [ 0.01662927],\n",
       "        [ 0.00887311],\n",
       "        [-0.01498431]], dtype=float32)>,\n",
       " <tf.Variable 'dense_336/bias:0' shape=(1,) dtype=float32, numpy=array([0.6126989], dtype=float32)>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.loss_weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'mse', 'mape']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.metrics.MeanMetricWrapper at 0x24a8118b088>,\n",
       " <keras.metrics.MeanMetricWrapper at 0x24abe6c2dc8>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the dropout rate\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasRegressor\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(500, input_dim=279, kernel_initializer='uniform', activation='tanh', kernel_constraint=maxnorm(weight_constraint)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='tanh'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=16, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
